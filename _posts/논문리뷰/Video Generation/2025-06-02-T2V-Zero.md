---
layout: single
title: "[논문리뷰] Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"
last_modified_at: 2025-06-02
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: "ICCV 2023"
use_math: true
toc: true
toc_sticky: true
---

> [[Paper]](https://arxiv.org/abs/2303.13439)
> 
> **ICCV 2023**

## Introduction

- 이전의 연구들은 상당한 양의 labeled 데이터가 필요로 했디.
- Tune-A-Video 논문에서는 비디오 도메인에서 Stable Diffusion 모델을 적용하여 단 하나의 비디오만 튜닝함으로써 학습 비용을 크게 줄였지만 여전히 최적화 과정이 필요하다.
- 제안하는 방법은 2개의 lightweight modification을 통해 어떠한 파인튜닝이나 최적화도 필요없이 사전학습된 T2I 모델만 사용할 수 있다.
  - Enrich the latent codes of generated frames with motion information
  - 첫 번째 프레임에 대해 Use cross-frame attention of each frame on the first frame

## Methods

가장 단순한 방법은 $x_T\sim\mathcal{N}(0,I)$을, $x_0$를 얻기 위해 DDIM 샘플링을 적용하는 것이다.

하지만 이 방법은 주어진 텍스트 묘사 $\tau$에만 관련되어 있는 랜덤한 이미지를 생성한다.
