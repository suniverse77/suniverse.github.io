---
layout: single
title: "[논문리뷰] Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"
last_modified_at: 2025-06-02
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: "ICCV 2023"
use_math: true
toc: true
toc_sticky: true
---

> [[Paper]](https://arxiv.org/abs/2303.13439)
> 
> **ICCV 2023**

## Introduction

- 이전의 연구들은 상당한 양의 labeled 데이터가 필요로 했디.
- Tune-A-Video 논문에서는 비디오 도메인에서 Stable Diffusion 모델을 적용하여 단 하나의 비디오만 튜닝함으로써 학습 비용을 크게 줄였지만 여전히 최적화 과정이 필요하다.
- 제안하는 방법은 2개의 lightweight modification을 통해 어떠한 파인튜닝이나 최적화도 필요없이 사전학습된 T2I 모델만 사용할 수 있다.
  - 생성된 프레임들의 latent code를 모션 정보로 보강
  - 각 프레임에서 첫 번째 프레임에 대한 cross-frame attention 사용

## Methods

<center><img src='{{"/assets/images/논문리뷰/T2VZ-2.png" | relative_url}}' width="100%"></center>

- Motion dynamic은 global scene와 background의 시간적 일관성을 향상시킨다.
- Cross-Frame Attention은 foreground 객체의 외관, 형태, 정체성을 보존한다.
- Background Smoothing은 background의 시간적 일관성을 향상시킨다. (필요하면 추가적으로 사용하는 모듈임)

### Motion Dynamics in Latent Codes

가장 단순한 방법은 $x_T\sim\mathcal{N}(0,I)$을, $x_0$를 얻기 위해 DDIM 샘플링을 적용하는 것이다.

하지만 이 방법은 주어진 텍스트 묘사 $\tau$에만 관련되어 있는 랜덤한 이미지를 생성한다.

<center><img src='{{"/assets/images/논문리뷰/T2VZ-1.png" | relative_url}}' width="100%"></center>

### Reprogramming Cross-Frame Attention

$$
Cross-Frame-Attn(Q^k,K^{1:m},V^{1:m})=\text{Softmax}(\frac{Q^k(K^1)^\top}{\sqrt{c}})V^1
$$

기존의 self-attention layer를 위와 같이 정의된 cross-frame-attention layer로 수정하였다.

첫 번째 프레임에서 생성된 객체를 다른 프레임에서도 유지하기 위해 각 프레임을 첫 번째 프레임과 cross-attention 연산을 수행한다.

### Background Smoothing (Optional)

$$
\bar{x}_t^k=M^kx_t^k+(1-M^k)(\alpha\hat{x}_t^k+(1-\alpha)x_t^k)
$$

### Conditional and Specialized Text-to-Video

<center><img src='{{"/assets/images/논문리뷰/T2VZ-3.png" | relative_url}}' width="100%"></center>

## Experiments

<center><img src='{{"/assets/images/논문리뷰/T2VZ-4.png" | relative_url}}' width="100%"></center>

<center><img src='{{"/assets/images/논문리뷰/T2VZ-5.png" | relative_url}}' width="100%"></center>
