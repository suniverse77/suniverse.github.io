---
layout: single
title: "[논문리뷰] Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators"
last_modified_at: 2025-06-02
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: "ICCV 2023"
use_math: true
toc: true
toc_sticky: true
---

> [[Paper]](https://arxiv.org/abs/2303.13439)
> 
> **ICCV 2023**

## Introduction

- 기존 연구들에서는 상당한 양의 labeled 데이터가 필요로 했다.
- Tune-A-Video 논문에서는 비디오 도메인에서 Stable Diffusion 모델을 적용하여 단 하나의 비디오만 튜닝함으로써 학습 비용을 크게 줄였지만 여전히 최적화 과정이 필요하다.
- 제안하는 방법은 2개의 lightweight modification을 통해 어떠한 파인튜닝이나 최적화도 필요없이 사전학습된 T2I 모델만 사용할 수 있다.
  - 생성된 프레임들의 latent code를 모션 정보로 보강
  - 각 프레임에서 첫 번째 프레임에 대한 cross-frame attention 사용

## Methods

<center><img src='{{"/assets/images/논문리뷰/T2VZ-1.png" | relative_url}}' width="100%"></center>

- Motion dynamic은 global scene와 background의 시간적 일관성을 향상시킨다.
- Cross-Frame Attention은 foreground 객체의 외관, 형태, 정체성을 보존한다.
- Background Smoothing은 background의 시간적 일관성을 향상시킨다. (필요하면 추가적으로 사용하는 모듈임)

### 1. Motion Dynamics in Latent Codes

각 프레임에 대한 초기 노이즈 $x^1_T,\cdots x^m_T$를 생성하는 가장 단순한 방법은 프레임마다 독립적으로 $\mathcal{N}(0,I)$에서 샘플링하는 것이다. 하지만 이런 방식의 샘플링은 주어진 텍스트와는 관련이 있을 수 있지만, 프레임 간 시공간적 일관성이 없는 랜덤한 이미지들만 생성하게 된다.

따라서 제안하는 motion dynamic을 이용하면 global scene의 시간적 일관성이 있는 latent code들을 생성할 수 있다.

프레임마다 초기 노이즈를 랜덤하게 샘플링하는 대신, 첫 번째 프레임의 잠재 코드를 무작위로 샘플링한 뒤 연산을 통해 움직임을 부여하고 나머지 프레임들을 생성함으로써 더 일관된 프레임 시퀀스를 생성한다.

<center><img src='{{"/assets/images/논문리뷰/T2VZ-2.png" | relative_url}}' width="70%"></center>

1. 첫 번째 프레임의 latent code를 샘플링한다.
  
   $$
   x_T^1\sim\mathcal{N}(0,I)
   $$
2. 첫 번째 프레임에 대해 $\Delta t=T-T'$ step의 DDIM backward를 적용한다.
  
   $$
   x_T^1\rightarrow x_{T'}^1
   $$
3. $k=2,3,\dots,m$에 대해 아래의 과정을 반복하면서 모든 프레임에 대한 latent 얻는다.
4. Global translation 벡터를 정의한다. (프레임이 뒤로 갈수록 첫 번째 프레임과의 움직임 차이가 커짐)
  
   $$
   \delta_k=\lambda\cdot(k-1)\delta~,~\text{where }\delta=(\delta_x,\delta_y)\in\mathbb{R}^2
   $$
5. Warping operation을 정의한다. ($\delta_k$를 이용해 입력을 translation하는 연산)

   $$
   W_k(\cdot)
   $$
6. 첫 번째 프레임에 warping operation을 적용해, $k$번째 프레임 대한 latent를 얻는다.

   $$
   \tilde{x}_{T'}^k=W_k(\tilde{x}_{T'}^1)
   $$
7. $k$번째 프레임에 $\Delta t$ step의 DDPM forward를 적용해 latent code를 얻는다.
  
   $$
   \tilde{x}_{T'}^k\rightarrow x_T^k
   $$
   

### 2. Reprogramming Cross-Frame Attention

<center><img src='{{"/assets/images/논문리뷰/T2VZ-3.png" | relative_url}}' width="80%"></center>

$$
\text{Cross-Frame-Attn}(Q^k,K^{1:m},V^{1:m})=\text{Softmax}(\frac{Q^k(K^1)^\top}{\sqrt{c}})V^1
$$

기존의 self-attention layer를 위와 같이 정의된 cross-frame-attention layer로 수정하였다. ($k$는 프레임 번호를 의미)

첫 번째 프레임에서 생성된 객체를 다른 프레임에서도 유지하기 위해 각 프레임을 첫 번째 프레임과 cross-attention 연산을 수행한다.

### 3. Background Smoothing (Optional)

이 방법은 텍스트 프롬프트만 주어지고 초기 이미지가 없거나 1장만 제공되는 경우에 효과적이다.

초기 이미지가 부족하거나 없는 경우에는 모든 프레임을 텍스트만으로 일관되게 생성하기 어렵기 때문에 배경이 흔들리거나 변형될 수가 있다. 따라서 아래의 방법을 사용해 첫 번째 프레임의 배경 정보를 뒤의 프레임에도 지속적으로 반영하도록 한다.

$$
\bar{x}_t^k=M^k\odot x_t^k+(1-M^k)\odot(\alpha\hat{x}_t^k+(1-\alpha)x_t^k)
$$

Mask에서 foreground인 영역에는 현재 프레임의 latent code를 그대로 사용해 객체의 형태를 보존한다.

Mask에서 background인 영역에는 첫 프레임을 warping한 latent와 현재 프레임 latent의 convex combination를 통해 배경의 변화를 일관적이고 부드럽게 반영한다.

- $x_t^k$ : $k$번째 latent code
- $\hat{x}_t^k$ : $x_t^1$에 대해 warping operation $W_k$를 적용한 latent code
- $M^k$ : Salient objection detection을 적용해 얻은 foreground mask

### 4. Conditional and Specialized Text-to-Video

<center><img src='{{"/assets/images/논문리뷰/T2VZ-4.png" | relative_url}}' width="70%"></center>

ControlNet을 사용해 condition을 줄 수도 있다.

## Experiments

<figure style="text-align: center;">
  <img src='{{ "/assets/images/논문리뷰/T2VZ-5.png" | relative_url }}' width="100%">
  <figcaption>text-to-video 생성</figcaption>
</figure>

<figure style="text-align: center;">
  <img src='{{ "/assets/images/논문리뷰/T2VZ-6.png" | relative_url }}' width="100%">
  <figcaption>다양한 condition에 따른 비디오 생성</figcaption>
</figure>
