---
layout: single
title: "[논문리뷰] Make-A-Video: Text-to-Video Generation without Text-Video Data"
last_modified_at: 2025-06-01
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: ""
use_math: true
toc: true
toc_sticky: true
---

> paper: 

## Introduction

- 텍스트와 이미지 사이의 연관성을 학습하기 위해 T2I 모델을 사용했다.
- Realitic motion을 학습하기 위해 unlabeled 비디오 데이터를 기반으로 비지도 학습을 수행했다.
- Paired text-video 데이터 없이도 텍스트로부터 비디오 생성이 가능하다.
- 기존 T2I 모델의 지식을 시간 정보를 포함한 spatiotemporal 네트워크로 확장하여 빠른 T2V 모델의 학습이 가능하다.

## Methods

<center><img src='{{"/assets/images/논문리뷰/MAV-1.png" | relative_url}}' width="100%"></center>

$$
\hat{y}_t=\text{SR}_h\circ\text{SR}_l^t\circ\uparrow_F\circ D^t\circ P\circ(\hat{x},C_x(x))
$$

$x$는 input text, $\hat{x}$는 BPE-encoded text, $C_x(x)$는 CLIP text encoder이다.

- Prior network $P$는 text embedding과 text token을 입력 받아 image embedding을 출력한다.
- Decoder network $D$는 image embedding을 조건으로 받아 64x64 해상도의 RGB 이미지를 출력한다.
- Super-resolution network $\text{SR}_l,\text{SR}_h$는 64x64 해상도의 이미지를 입력 받아 각각 256x256 해상도, 768x768 해상도의 RGB 이미지를 출력한다.

### SpatioTemporal Layers

비디오를 생성하기 위해서는 spatial한 차원뿐만 아니라 temporal한 차원까지 고려해야하기 때문에 대부분의 U-Net 기반 디퓨전 네트워크들을 수정하였다.

- spatiotemporal decoder $D^t$는 64x64 크기의 RGB 프레임 16장을 생성
- frame interpolation network $\uparrow_F$는 디코더가 생성한 16장의 프레임 사이를 보간해 frame rate를 증가시킴
- super-resolution networks $\text{SR}^t_l$

FC layer와 같은 layer들은 상관없기 때문에 추가적인 수정은 안했다고 한다.

<center><img src='{{"/assets/images/논문리뷰/MAV-2.png" | relative_url}}' width="100%"></center>

#### Pseudo-3D Convolutional Layers

일반적인 3D conv를 사용하지 않고, pseudo-3D conv 방식을 사용하였다.

$$
Conv_{P3D}(h):=Conv_{1D}(Conv_{2d}(h)\circ T)\circ T
$$

Psuedo-3D conv는 위와 같이 정의되며, $\circ T$는 spatial 차원과 temporal 차원을 transpose하는 연산을 의미한다.

1. 2D conv를 이용해 각 프레임별로 feature map을 추출
2. 추출된 feature map을 프레임별로 채널축으로 쌓음
3. 1D conv를 이용해 프레임별로 연산을 진행

```python
b, c, *_, h, w = x.shape

is_video = x.ndim == 5
enable_time &= is_video

if is_video:
    x = rearrange(x, 'b c f h w -> (b f) c h w')

x = self.spatial_conv(x)

if is_video:
    x = rearrange(x, '(b f) c h w -> b c f h w', b = b)

if not enable_time or not exists(self.temporal_conv):
    return x

x = rearrange(x, 'b c f h w -> (b h w) c f')

x = self.temporal_conv(x)

x = rearrange(x, '(b h w) c f -> b c f h w', h = h, w = w)
```

#### Pseudo-3D Attention Layers

Pseudo-3D Conv와 동일하게 차원 분해 전략을 attention layer에도 적용하였다.

$$
ATTN_{P3D}(h)=unflatten(ATTN_{1D}(ATTN_{2d}(flatten(h))\circ T)\circ T)
$$

입력 텐서가 $\mathbb{R}^{B\times C\times F\times H\times W}$인 이유는 PyTorch의 nn.Conv3d가 (B, C, D, H, W) 형태의 입력을 받기 때문이다.

```python
b, c, *_, h, w = x.shape
is_video = x.ndim == 5
enable_time &= is_video

if is_video:
    x = rearrange(x, 'b c f h w -> (b f) (h w) c')
else:
    x = rearrange(x, 'b c h w -> b (h w) c')

space_rel_pos_bias = self.spatial_rel_pos_bias(h, w) if exists(self.spatial_rel_pos_bias) else None

x = self.spatial_attn(x, rel_pos_bias = space_rel_pos_bias) + x

if is_video:
    x = rearrange(x, '(b f) (h w) c -> b c f h w', b = b, h = h, w = w)
else:
    x = rearrange(x, 'b (h w) c -> b c h w', h = h, w = w)

if enable_time:

    x = rearrange(x, 'b c f h w -> (b h w) f c')

    time_rel_pos_bias = self.temporal_rel_pos_bias(x.shape[1]) if exists(self.temporal_rel_pos_bias) else None

    x = self.temporal_attn(x, rel_pos_bias = time_rel_pos_bias) + x

    x = rearrange(x, '(b h w) f c -> b c f h w', w = w, h = h)

if self.has_feed_forward:
    x = self.ff(x, enable_time = enable_time) + x
```

### Frame Interpolation Network
