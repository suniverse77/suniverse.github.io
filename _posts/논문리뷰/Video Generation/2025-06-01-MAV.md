---
layout: single
title: "[논문리뷰] Make-A-Video: Text-to-Video Generation without Text-Video Data"
last_modified_at: 2025-06-01
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: ""
use_math: true
toc: true
toc_sticky: true
---

> paper: 

## Introduction

- 텍스트와 이미지 사이의 연관성을 학습하기 위해 T2I 모델을 사용했다.
- Realitic motion을 학습하기 위해 unlabeled 비디오 데이터를 기반으로 비지도 학습을 수행했다.
- Paired text-video 데이터 없이도 텍스트로부터 비디오 생성이 가능하다.
- 기존 T2I 모델의 지식을 시간 정보를 포함한 spatiotemporal 네트워크로 확장하여 빠른 T2V 모델의 학습이 가능하다.

## Methods

<center><img src='{{"/assets/images/논문리뷰/MAV-1.png" | relative_url}}' width="100%"></center>

$$
\hat{y}_t=\text{SR}_h\circ\text{SR}_l^t\circ\uparrow_F\circ D^t\circ P\circ(\hat{x},C_x(x))
$$

$x$는 input text, $\hat{x}$는 BPE-encoded text, $C_x(x)$는 CLIP text encoder이다.

- Prior network $P$는 text embedding과 text token을 입력 받아 image embedding을 출력한다.
- Decoder network $D$는 image embedding을 조건으로 받아 64x64 해상도의 RGB 이미지를 출력한다.
- Super-resolution network $\text{SR}_l,\text{SR}_h$는 64x64 해상도의 이미지를 입력 받아 각각 256x256 해상도, 768x768 해상도의 RGB 이미지를 출력한다.

### SpatioTemporal Layers

비디오를 생성하기 위해서는 spatial한 차원뿐만 아니라 temporal한 차원까지 고려해야하기 때문에 대부분의 U-Net 기반 디퓨전 네트워크들을 수정하였다.

- spatiotemporal decoder $D^t$는 64x64 크기의 RGB 프레임 16장을 생성
- frame interpolation network $\uparrow_F$는 디코더가 생성한 16장의 프레임 사이를 보간해 frame rate를 증가시킴
- Super-resolution networks $\text{SR}^t_l$는 공간과 시간 차원에서 작동하며, $\text{SR}_h$는 메모리 제약, 고해상도 비디오 데이터의 부족으로 인해 공간 차원에서만 작동함

FC layer와 같은 layer들은 상관없기 때문에 추가적인 수정은 안했다고 한다.

<center><img src='{{"/assets/images/논문리뷰/MAV-2.png" | relative_url}}' width="100%"></center>

#### Pseudo-3D Convolutional Layers

일반적인 3D conv를 사용하지 않고, pseudo-3D conv라 방식을 사용하였다.

$$
Conv_{P3D}(h):=Conv_{1D}(Conv_{2d}(h)\circ T)\circ T
$$

Psuedo-3D conv는 위와 같이 정의되며, $\circ T$는 spatial 차원과 temporal 차원을 transpose하는 연산을 의미한다.

1. 2D conv를 이용해 각 프레임별로 feature map을 추출
2. 추출된 feature map을 프레임별로 채널축으로 쌓음
3. 1D conv를 이용해 프레임별로 연산을 진행

각 컨볼루션 연산 전, 입력 텐서의 차원은 아래와 같다.

- $Conv_{2d}$ 입력 전: $(BF\times C\times H\times W)$
- $Conv_{1d}$ 입력 전: $(BHW\times C\times F)$

#### Pseudo-3D Attention Layers

Pseudo-3D Conv와 동일하게 차원 분해 전략을 attention layer에도 적용하였다.

$$
ATTN_{P3D}(h)=unflatten(ATTN_{1D}(ATTN_{2d}(flatten(h))\circ T)\circ T)
$$

$flatten$은 공간 차원으로 평탄화시키는 연산이다. $h\in\mathbb{R}^{B\times C\times F\times H\times W}\rightarrow h\in\mathbb{R}^{B\times C\times F\times HW}$

각 attention 연산 전, 입력 텐서의 차원은 아래와 같다.

- $ATTN_{2d}$ 입력 전: $(BF\times HW\times C)$
- $ATTN_{1d}$ 입력 전: $(BHW\times F\times C)$

#### Frame rate conditioning

CogVideo에서 사용한 방식과 유사하게, fps를 컨디션으로 줌

16개의 프레임을 1~30fps 범위에서 랜덤으로 샘플링함 (높은 fps범위에서 샘플링한 후, 학습을 진행할수록 낮은 fps 범위에서 샘플링하도록 함)

### Frame Interpolation Network

원래 이미지는 그대로 두고, 보간할 프레임 위치는 4채널로 구성하였음 (원래 RGB 이미지 3채널 + binary 이미지 1채널)

### Training

모델의 각 구성요소들은 서로 독립적으로 학습된다.

- 학습 때 Prior network $P$는 텍스트를, decoder $D^t$는 CLIP 이미지 임베딩을, Super-resolution components는 다운샘플된 이미지를 입력으로 받음
- $P$, $D^t$, $\text{SR}^t_l$, $\text{SR}_h$들을 먼저 이미지 데이터에 대해서 학습시킴
- 이후 temporal layer를 추가하고 $D^t$, $\text{SR}^t_l$, $\text{SR}_h$들만 unlabeled 비디오 데이터에 대해서 파인튜닝함

## Experiments

