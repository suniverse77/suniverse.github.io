---
layout: single
title: "[논문리뷰] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
last_modified_at: 2025-06-03
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: "CVPR 2023"
use_math: true
toc: true
toc_sticky: true
---

> [[Paper]](https://arxiv.org/abs/2304.08818)
> 
> **CVPR 2023**

## Introduction

**기존 연구들의 한계점**
- 비디오 데이터 학습에는 막대한 계산 자원이 필요하고, 공개된 대규모 비디오 데이터셋의 부족 때문에 비디오 생성 모델링의 발전이 더디다.
- 여러 연구들이 진행되었지만, 대부분 저해상도이거나 짧은 길이의 비디오만 생성 가능하다.

**제안하는 방법**
- Latent Diffusion Model (LDM)을 사용함으로써 고해상도 및 장시간 비디오 생성을 가능하게 한다.
- 사전 학습된 이미지 생성 LDM을 비디오 생성 LDM으로 확장하기 위해 시간 계층을 추가하였다.
- 공간 계층은 고정하고 시간 계층만 학습하므로 효율적이다.
- 기존의 이미지 super-resolution에 널리 사용되던 pixel-space upsampler와 latente DM upsampler를 시간적으로 정렬시켜, 시간적으로 일관된 비디오 super-resolution 모델로 확한다.

## Methods

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-4.png" | relative_url}}' width="100%"></center>

### 1. Turning Latent Image into Video Generators

아래의 방법을 사용하면 대규모 이미지 데이터셋을 활용하여 공간 계층을 사전 학습할 수 있고, 상대적으로 덜 확보된 비디오 데이터는 시간 계층 학습에만 집중적으로 사용할 수 있다는 장점이 있다.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-3.png" | relative_url}}' width="100%"></center>

위 그림에서 $l_\theta^i$는 $i$번째 spatial layer, $l_\phi^i$는 $i$번 temporal layer, $\mathbf{c}$는 텍스트와 같은 conditioning을 의미한다.

$l_\theta^i$만으로는 temporal awareness(시간 흐름에 대한 이해)가 없기 때문에 비디오 생성에 바로 사용할 수 없다. 따라서 $l_\phi^i$를 도입해 개별 프레임들을 시간적으로 일관되게 정렬하는 방법을 학습한다.

프레임 수가 $T$인 비디오 입력 $\mathbf{z}\in\mathbb{R}^{B~T\times C\times H\times W}$는 spatial layer에서 $T$개의 독립적인 이미지 배치로 처리된다. 이후 temporal layer로 전달되기 전에 텐서의 크기는 $\mathbf{z'}\in\mathbb{R}^{B\times C\times T\times H\times W}$으로 재구성된다.

spatial layer의 출력 $\mathbf{z}\_i$와 temporal layer의 출력 $\mathbf{z'}\_i$는 $\mathbf{z}\_{i+1}=\alpha\_\phi^i\mathbf{z}+(1-\alpha_\phi^i)\mathbf{z'}$ 형태로 결합되어 다음 layer로 전달된다. ($\alpha$는 학습 가능한 파라미터임)

Conv3D layer, temporal attention layer 두 가지 유형의 temporal mixing layers을 구현하였다.

temporal layer는 기존 이미지 모델과 동일한 노이즈 스케줄을 사용하여 학습되며, 목적 함수는 아래와 같다.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-5.png" | relative_url}}' width="70%"></center>

여기서 $\mathbf{z}_\tau$는 노이즈가 추가된 latent를 의미한다.

#### Temporal Autoencoder Finetuning

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-2.png" | relative_url}}' width="80%"></center>

- 그림에서 위 부분은 temporal 디코더를 파인튜닝하는 과정을 나타내며, video-aware discriminator를 도입해 비디오의 시간적 일관성을 고려하도록 하였다. (GAN의 discriminator와는 다름)
- 그림에서 아래 부분은 latent space에서 LDM의 denoising 과정을 나타낸다. (학습한 분포에서 샘플링된 latent code를 디코더에 통과시키면 특정 이미지가 출력) 

### 2. Prediction Models for Long-Term Generation

1절에서서 설명한 방법은 짧은 비디오 시퀀스 생성에는 효과적이지만, 장시간 비디오를 생성하는 데에는 한계가 있다. 따라서 초기 $S$개의 context frame(이후 프레임을 생성할 때 참고하는 기준 프레임)이 주어졌을 때, 이를 기반으로 다음 프레임들을 생성하는 모델도 함께 학습한다.

이를 위해 전체 시퀀스 길이 $T$ 중에서 $T-S$개의 프레임을 마스킹하는 temporal binary mask $\mathbf{m}_S$를 도입하였으며, 아래와 같은 방법으로 모델에 조건으로 함께 입력된다.

$$
\mathbf{c}_S=(\mathbf{m}_S\circ\mathbf{z},\mathbf{m}_S)
$$

- latent $\mathbf{z}$와 mask $\mathbf{m}_S$를 곱한다.
- $\mathbf{m}_S\circ\mathbf{z}$를 mask $\mathbf{m}_S$와 함께 채널 방향으로 결합한다.
- 만들어진 $\mathbf{c}_S$는 모델에 입력되기 전 downsampling을 진행한 후 2D conv layer를 거친다.

이때의 학습 목적 함수는 아래와 같으며, 실제로는 0, 1, 2개의 context frame에 대해 예측하는 모델을 학습한다..

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-6.png" | relative_url}}' width="60%"></center>

#### inference

아래와 같은 방법으로 이전에 예측한 결과를 새로운 context로 재사용함으로써 장시간 비디오를 생성할 수 있다.

1. 이미지 생성 모델을 사용해 1개의 context frame을 생성한다.
2. 이를 기반으로 2번째 프레임을 생성한다.
3. 이후에는 2개의 context frame을 기반으로 움직임을 인코딩하여 생성한다.

위 과정을 안정화시키기 위해, classifier-free diffusion guidance를 사용하였고, 이때의 샘플링 방식은 아래와 같다.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-7.png" | relative_url}}' width="70%"></center>

$s\geq1$은 guidance scale을 의미하며, 이러한 방법의 guidance를 context guidance라고 부른다.

### 3. Temporal Interpolation for High Frame Rates

고해상도 비디오는 spatial resolution뿐만 아니라 temporal resolution 또한 중요하기 때문에, 비디오 생성 과정을 두 단계로 나누어 수행하였다.

**1단계**

1절과 2절에서 설명한 방법으로, 의미적으로 큰 변화가 있는 key frame들을 생성하는 단계이다. 그러나 이 방식은 메모리 제약으로 인해 비교적 낮은 프레임 레이트로만 생성이 가능하다.

**2단계**

key frame 사이를 보간하는 추가적인 모델을 도입한다. 이를 구현하기 위해 2절에서 제안한 masking-conditioning 메커니즘을 그대로 활용한다. 그러나 예측 과제와 달리, 이번에는 보간 대상인 프레임들을 마스킹한다는 점이 다르다. 나머지 메커니즘은 동일하며, 즉 기존의 이미지 생성 모델을 비디오 보간 모델로 확장하는 것이다.

실제 실험에서는 $T\rightarrow 4T$ 보간 모델을 학습시키며, 더 높은 frame rate를 위해 binary conditioning을 통해 $T\rightarrow 4T$와 $4T\rightarrow 16T$를 동시에 학습시켰다.

예측 및 보간 모델 학습 방식은 유사한 마스킹 기법을 사용하는 최근 연구들에서 영감을 받았다고 한다.

- [Flexible Diffusion Modeling of Long Videos](https://arxiv.org/abs/2205.11495)
- [Diffusion Models for Video Prediction and Infilling](https://arxiv.org/abs/2206.07696)
- [MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation](https://arxiv.org/abs/2205.09853)

### 4. Temporal Fine-tuning of SR Models

LDM의 출력을 메가픽셀 단위로 향상시키기 위해 cascaded DMs에서 영감을 받아, Video LDM의 출력에 대해 추가 확산 모델을 적용하여 4배 업샘플링하였다.

noise-level conditioning을 활용하여 super-resolution 모델 $\mathbf{g}_{\theta,\phi}$을 학습하며, 목적 함수는 아래와 같다.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-8.png" | relative_url}}' width="70%"></center>

여기서 $\mathbf{c}\_{\tau_\gamma}=\alpha_{\tau_\gamma}\mathbf{x}+\sigma_{\tau_\gamma}\epsilon$는 노이즈가 추가된 저해상도 이미지를 의미한다.

비디오 프레임을 각각 독립적으로 업샘플링하면 시간적 일관성이 떨어지므로, super-resolution 모델 또한 video-aware으로 만든다.
구체적으로는 1절에서 소개한 메커니즘을 따라 spaital layer와 temporal layer를 포함하고, 업샘플러를 비디오 시퀀스 단위로 파인튜닝한다.

저자들은 LDM 기반 비디오 생성기와 업샘플링 확산 모델의 조합이 고해상도 비디오 생성을 위한 이상적인 구성이라고 한다.

- 모든 예측과 보간이 latent space에서 수행되기 때문에 큰 배치 사이즈로 더 많은 프레임을 한꺼번에 인코딩 가능 (장기 비디오 생성에 유리)
- 업샘플러는 전체 프레임을 한 번에 처리하지 않고, 이미지를 patch 단위로 나누어 학습하기 때문에 고해상도의 비디오도 효율적으로 처리 가능
- 업샘플러는 이미 어느 정도 시간적으로 일관된 저해상도 프레임을 조건으로 받기 때문에 시간 흐름을 따로 학습할 필요 없이 단순히 이미지를 더 선명하게 만들기만 하면 됨 (별도의 예측/보간 프레임워크가 필요 없음)

## Experiments

### High-Resolution Driving Video Synthesis

RDS (Real Driving Scene) 데이터셋 사용해 먼저 Image LDM을 개별 프레임에 대해 학습한 뒤, temporal layer를 비디오 전체에 대해 학습.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-10.png" | relative_url}}' width="90%"></center>


### Text-to-Video with Stable Diffusion

WebVid-10M 데이터셋을 사용해 먼저 프레임 레벨로 파인튜닝한 뒤, temporal layer를 삽입하여 학습

<figure style="text-align: center;">
  <img src='{{ "/assets/images/논문리뷰/VideoLDM-9.png" | relative_url }}' width="70%">
  <figcaption>Video LDM으로 생성한 1280×2048 해상도의 샘플</figcaption>
</figure>

Dreambooth로 개인화된 텍스트-비디오 생성
