---
layout: single
title: "[논문리뷰] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
last_modified_at: 2025-06-03
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: "CVPR 2023"
use_math: true
toc: true
toc_sticky: true
---

> [[Paper]](https://arxiv.org/abs/2304.08818)
> 
> **CVPR 2023**

## Introduction

**기존 연구들의 한계점**
- 비디오 데이터 학습에는 막대한 계산 자원이 필요하고, 공개된 대규모 비디오 데이터셋의 부족 때문에 비디오 생성 모델링의 발전이 더디다.
- 여러 연구들이 진행되었지만, 대부분 저해상도이거나 짧은 길이의 비디오만 생성 가능하다.

**제안하는 방법**
- Latent Diffusion Model (LDM)을 사용함으로써 고해상도 및 장시간 비디오 생성을 가능하게 한다.
- 사전 학습된 이미지 생성 LDM을 비디오 생성 LDM으로 확장하기 위해 시간 계층을 추가하였다.
- 공간 계층은 고정하고 시간 계층만 학습하므로 효율적이다.
- 기존의 이미지 super-resolution에 널리 사용되던 pixel-space upsampler와 latente DM upsampler를 시간적으로 정렬시켜, 시간적으로 일관된 비디오 super-resolution 모델로 확한다.

## Methods

### Turning Latent Image into Video Generators

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-3.png" | relative_url}}' width="100%"></center>

위 그림에서 $l_\theta^i$는 $i$번째 spatial layer, $l_\phi^i$는 $i$번 temporal layer, $\mathbf{c}$는 텍스트와 같은 conditioning을 의미한다.

$l_\theta^i$만으로는 temporal awareness(시간 흐름에 대한 이해)가 없기 때문에 비디오 생성에 바로 사용할 수 없다. 따라서 $l_\phi^i$를 도입해 개별 프레임들을 시간적으로 일관되게 정렬하는 방법을 학습한다.

프레임 수가 $T$인 비디오 입력 $\mathbf{z}\in\mathbb{R}^{BT\times C\times H\times W}$는 spatial layer에서 $T$개의 독립적인 이미지 배치로 처리된다. 이후 temporal layer로 전달되기 전에 텐서의 크기는 $\mathbf{z'}\in\mathbb{R}^{B\times C\times T\times H\times W}$으로 재구성된다.

spatial layer의 출력 $\mathbf{z}_i$와 temporal layer의 출력 $\mathbf{z'}_i$는 $\mathbf{z}{i+1}=\alpha\phi^i\mathbf{z}+(1-\alpha_\phi^i)\mathbf{z'}$ 형태로 결합되어 다음 layer로 전달된다. ($\alpha$는 학습 가능한 파라미터임)

### Prediction Models for Long-Term Generation

### Temporal Interpolation for High Frame Rates

### Temporal Fine-tuning of SR Models

## Experiments


