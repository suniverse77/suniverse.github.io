---
layout: single
title: "[논문리뷰] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
last_modified_at: 2025-06-03
categories: ["논문리뷰"]
tags: ["Video Generation"]
excerpt: "CVPR 2023"
use_math: true
toc: true
toc_sticky: true
---

> [[Paper]](https://arxiv.org/abs/2304.08818)
> 
> **CVPR 2023**

## Introduction

**기존 연구들의 한계점**
- 비디오 데이터 학습에는 막대한 계산 자원이 필요하고, 공개된 대규모 비디오 데이터셋의 부족 때문에 비디오 생성 모델링의 발전이 더디다.
- 여러 연구들이 진행되었지만, 대부분 저해상도이거나 짧은 길이의 비디오만 생성 가능하다.

**제안하는 방법**
- Latent Diffusion Model (LDM)을 사용함으로써 고해상도 및 장시간 비디오 생성을 가능하게 한다.
- 사전 학습된 이미지 생성 LDM을 비디오 생성 LDM으로 확장하기 위해 시간 계층을 추가하였다.
- 공간 계층은 고정하고 시간 계층만 학습하므로 효율적이다.
- 기존의 이미지 super-resolution에 널리 사용되던 pixel-space upsampler와 latente DM upsampler를 시간적으로 정렬시켜, 시간적으로 일관된 비디오 super-resolution 모델로 확한다.

## Methods

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-2.png" | relative_url}}' width="80%"></center>

- 그림에서 위 부분은 temporal 디코더를 파인튜닝하는 과정을 나타내며, video-aware discriminator를 도입해 비디오의 시간적 일관성을 고려하도록 하였다. (GAN의 discriminator와는 다름)
- 그림에서 아래 부분은 latent space에서 LDM의 denoising 과정을 나타낸다. (학습한 분포에서 샘플링된 latent code를 디코더에 통과시키면 특정 이미지가 출력) 

### Turning Latent Image into Video Generators

아래의 방법을 사용하면 대규모 이미지 데이터셋을 활용하여 공간 계층을 사전 학습할 수 있고, 상대적으로 덜 확보된 비디오 데이터는 시간 계층 학습에만 집중적으로 사용할 수 있다는 장점이 있다.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-3.png" | relative_url}}' width="100%"></center>

위 그림에서 $l_\theta^i$는 $i$번째 spatial layer, $l_\phi^i$는 $i$번 temporal layer, $\mathbf{c}$는 텍스트와 같은 conditioning을 의미한다.

$l_\theta^i$만으로는 temporal awareness(시간 흐름에 대한 이해)가 없기 때문에 비디오 생성에 바로 사용할 수 없다. 따라서 $l_\phi^i$를 도입해 개별 프레임들을 시간적으로 일관되게 정렬하는 방법을 학습한다.

프레임 수가 $T$인 비디오 입력 $\mathbf{z}\in\mathbb{R}^{B~T\times C\times H\times W}$는 spatial layer에서 $T$개의 독립적인 이미지 배치로 처리된다. 이후 temporal layer로 전달되기 전에 텐서의 크기는 $\mathbf{z'}\in\mathbb{R}^{B\times C\times T\times H\times W}$으로 재구성된다.

spatial layer의 출력 $\mathbf{z}\_i$와 temporal layer의 출력 $\mathbf{z'}\_i$는 $\mathbf{z}\_{i+1}=\alpha\_\phi^i\mathbf{z}+(1-\alpha_\phi^i)\mathbf{z'}$ 형태로 결합되어 다음 layer로 전달된다. ($\alpha$는 학습 가능한 파라미터임)

Conv3D layer, temporal attention layer 두 가지 유형의 temporal mixing layers을 구현하였다.

temporal layer는 기존 이미지 모델과 동일한 노이즈 스케줄을 사용하여 학습되며, 목적 함수는 아래와 같다.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-5.png" | relative_url}}' width="70%"></center>

여기서 $\mathbf{z}_\tau$는 노이즈가 추가된 latent를 의미한다.

### Prediction Models for Long-Term Generation

위에서 설명한 방법은 짧은 비디오 시퀀스 생성에는 효과적이지만, 장시간 비디오를 생성하는 데에는 한계가 있다. 따라서 초기 $S$개의 context frame(이후 프레임을 생성할 때 참고하는 기준 프레임)이 주어졌을 때, 이를 기반으로 다음 프레임들을 생성하는 모델도 함께 학습한다.

이를 위해 전체 시퀀스 길이 $T$ 중에서 $T-S$개의 프레임을 마스킹하는 temporal binary mask $\mathbf{m}_S$를 도입하였으며, 아래와 같은 방법으로 모델에 조건으로 함께 입력된다.

$$
\mathbf{c}_S=(\mathbf{m}_S\circ\mathbf{z},\mathbf{m}_S)
$$

- latent $\mathbf{z}$와 mask $\mathbf{m}_S$를 곱한다.
- $\mathbf{m}_S\circ\mathbf{z}$를 mask $\mathbf{m}_S$와 함께 채널 방향으로 결합한다.
- 만들어진 $\mathbf{c}_S$는 모델에 입력되기 전 downsampling을 진행한 후 2D conv layer를 거친다.

이때의 학습 목적 함수는 아래와 같으며, 실제로는 0, 1, 2개의 context frame에 대해 예측하는 모델을 학습한다..

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-6.png" | relative_url}}' width="70%"></center>

#### inference

장기 비디오 생성을 위해, 아래와 같은 방법으로 이전에 예측한 결과를 새로운 컨텍스트로 재사용할 수 있다.

1. 기본 이미지 생성 모델을 사용해 1개의 컨텍스트 프레임을 생성
2. 이후에는, 2개의 컨텍스트 프레임을 기반으로 움직임을 인코딩하여 생성한다

이 과정을 안정화시키기 위해, 우리는 classifier-free diffusion guidance [30]를 사용하는 것이 효과적임을 발견했다. 이때의 샘플링 방식은 아래와 같다.

<center><img src='{{"/assets/images/논문리뷰/VideoLDM-7.png" | relative_url}}' width="70%"></center>

이 방법을 여기서 context guidance라고 부르며, $s\geq1$은 guidance scale을 의미한다.

### Temporal Interpolation for High Frame Rates

### Temporal Fine-tuning of SR Models

## Experiments


