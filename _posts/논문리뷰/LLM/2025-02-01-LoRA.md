---
layout: single
title: "[논문리뷰] LoRA: Low-Rank Adaptation of Large Language Models"
last_modified_at: 2025-02-01
categories: ["논문리뷰"]
tags: ["LLM"]
excerpt: "ICLR 2022"
use_math: true
toc: true
toc_sticky: true
---

**ICLR 2022** 
[[Paper]](https://arxiv.org/abs/2106.09685)

<details>
<summary><font color='#FF8C00'>📝 Summary</font></summary>
<div markdown="1">
<br>
파인튜닝은 전체 가중치를 업데이트해야 하지만, 실제로 업데이트할 때 필요한 가중치는 low-rank로 표현이 가능하다.
- 가중치 변화량을 2개의 작은 행렬로 분해 → $\Delta W=BA$
- 원래 가중치 $W_0$를 고정하고, $B$와 $A$만 학습

추론 시 가중치를 $W = W_0 + BA$ 하나로 합쳐서 사용하면 되기 때문에 추론 지연이 없다.

</div>
</details>

## Introduction

**기존 연구들의 한계점**

- 파인튜닝의 주요 단점은 새롭게 생성된 모델이 원래 모델과 동일한 수의 파라미터를 가진다는 점이다.
- 이러한 문제를 완화하기 위해 많은 연구들이 일부 파라미터만을 적응시키거나 새로운 작업을 위한 외부 모듈을 학습하는 방법을 제안했지만, 이러한 방법들은 효율성과 모델 품질 사이의 트레이드오프를 발생시킨다.

**제안하는 방법**

- 많은 파라미터를 가진 모델들(over-parametrized models)은 실제로 저차원에 놓여있다. → 이를 모델이 low intrinsic dimension을 가진다고 표현
- LoRA에서는 파인튜닝에서 발생하는 가중치 행렬 변화 $\Delta W$는 필요 이상으로 큰 행렬이며, 소수의 선형결합으로 표현할 수 있다고 가정하였다. → 이를 행렬이 low intrinsic rank를 가진다고 표현
    
    따라서, 사전학습된 원본 가중치 $W_0$는 고정시킨 채, 변화하는 dense layer의 가중치 $\Delta W$ 부분만 low rank 행렬 분해 형태로 학습한다.
    
- LoRA는 아래와 같은 장점을 가진다.
    1. 사전학습된 모델을 공유하는 여러 task에 대해서 저용량 행렬 $A$, $B$만 교체하면 되므로, task 간 전환이 효율적이다.
    2. 계산해야 할 gradient가 대폭 줄어들기 때문에 메모리 사용량이 절약된다. 이 덕분에 기존 모델을 학습했을 때보다 더 낮은 사양의 하드웨어에서도 학습이 가능하다.
    3. 단순한 선형 구조 덕분에, LoRA는 학습된 행렬을 고정된 가중치와 합쳐 배포할 수 있으며, 추론 지연이 전혀 없다.
    4. LoRA는 기존 방법들과 독립적으로 작동하기 때문에, prefix-tuning과 같은 다른 기법들과도 쉽게 결합할 수 있다.

## Methods

### 1. Problem Statement

사전학습된 autoregressive 언어 모델을 $P_\Phi(y\mid x)$라고 할 때, 파인튜닝 방식의 MLE 목적함수는 아래와 같다.

$$
\underset{\Phi}{\max}~
\sum_{(x,y)\in\mathcal{Z}}\sum_{t=1}^{\lvert y\rvert}
\log(P_\Phi(y_t\mid x,y_{<t}))
$$

$\Phi_0$를 사전학습된 가중치라고 할 때, 파인튜닝에서 파라미터 업데이트는 $\Phi_0+\Delta\Phi$로 표현된다. 이때, $\Phi_0$의 차원과 $\Delta\Phi$의 차원이 같기 때문에 거대 모델에서는 파인튜닝하는 것이 어렵다.

LoRA를 이용해 일부 파라미터만 업데이트하는 MLE 목적함수는 아래와 같다.

$$
\underset{\Theta}{\max}~
\sum_{(x,y)\in\mathcal{Z}}\sum_{t=1}^{\lvert y\rvert}
\log(P_{\Phi_0+\Delta\Phi(\Theta)}(y_t\mid x,y_{<t}))
$$

LoRA에서의 파라미터 업데이트는 $\Phi_0+\Delta\Phi(\Theta)$로 표현된다. 업데이트하는 파라미터의 크기 $\lvert\Theta\rvert$는 전체 파라미터의 크기 $\lvert\Phi_0\rvert$의 0.01%라고 한다.

### 2. Low-Rank-Parametrized Update Matrices

<center><img src='{{"/assets/images/논문리뷰/LoRA-1.png" | relative_url}}' width="40%"></center>
<br>
신경망에는 행렬 곱 연산을 수행하는 여러 개의 dense layer가 존재하며, dense layer의 가중치 행렬은 일반적으로 full-rank를 가진다.

기존 연구에서는 특정 작업에 적응(adaptation)할 때, 사전학습된 언어 모델이 low intrinsic dimension을 가지며, random projection을 통해 더 작은 subspace로 옮겨도 효율적으로 학습할 수 있음을 보였다.

이를 바탕으로, LoRA에서는 적응 과정에서 가중치 업데이트 또한 low intrinsic rank를 가진다고 가정하였으며, 업데이트 가중치 행렬 $\Delta W$를 low rank decomposition하였다. 

$$
W_0+\Delta W=W_0+BA
\\
W_0\in\mathbb{R}^{d\times k},~B\in\mathbb{R}^{d\times r},~A\in\mathbb{R}^{r\times k},~r\leq\min(d,k)
$$

학습 중에는 $W_0$를 고정하고, $A$와 $B$만 학습 가능한 파라미터로 둔다.

$\Delta W$와 $BA$ 모두 차원은 $d\times k$이지만, 업데이트할 파라미터 개수가 다르다.

- $\Delta W$는 full-rank이기 때문에 업데이트할 파라미터 개수가 $d\times k$개이다.
- $B$와 $A$는 low-rank이기 때문에 업데이트할 파라미터 개수가 $r(d+k)$개이다.

Forward process는 아래와 같이 정의된다.

$$
h=(W_0+\Delta W)x=W_0x+BAx
$$

행렬 $A$는 random Gaussian $\mathcal{N}(0,\sigma^2)$으로, 행렬 $B$는 0으로 초기화하여 학습 초기에는 $\Delta W=0$이 되도록 한다.

이후 $\Delta Wx$에 스케일링 상수 $\frac{\alpha}{r}$를 곱한다. ($\alpha$는 상수)

#### A Generalization of Full Fine-tuning

일반적인 파인튜닝은 사전학습된 파라미터의 일부분만 학습한다. LoRA는 더 나아가, 업데이트되는 가중치 행렬이 full-rank일 필요가 없도록 한다.

$r$을 사전학습된 가중치 행렬의 rank와 같게 설정하면, 전체를 파인튜닝하는 방식과 동일한 성능을 낼 수 있다.

#### No Additional Inference Latency

LoRA를 적용하면, 가중치를 합쳐서 $W=W_0+BA$로 미리 만들어둘 수 있다. 이렇게 하면 추론할 때 매번 $BAx$를 계산하지 않고 하나의 행렬곱 $Wx$만 수행하므로, 추론 속도가 느려지지 않는다.

다른 task로 전환해야 할 때는 $BA$를 빼고 다른 $B'A'$를 더하면 되므로 memory overhead가 거의 없다.

따라서 파인튜닝된 모델과 비교해 inference 때 추가적인 latency가 발생하지 않음을 보장할 수 있다.

### 3. Applying LoRA to Transformer

트랜스포머의 self-attention 모듈에는 4개의 가중치 행렬 $W_q,W_k,W_v,W_o$가 있고, MLP 모듈에는 2개의 가중치 행렬이 존재한다.

본 연구에서는 단순함과 파라미터 효율성의 이유로 MLP 모듈은 고정시키고, attention 모듈의 가중치만 LoRA로 적응시켰다.

#### Practical Benefits and Limitations

가장 큰 이점은 메모리와 저장 공간 사용량의 감소 및 task 간의 전환이 편리하다는 점이다.

서로 다른 task 샘플들을 하나의 배치로 묶었을 때, 각 task에 대응하는 $BA$를 미리 합쳐 하나의 $W$로 처리하는 것이 어렵기 때문에 배치 내 샘플 별로 분리된 연산이 필요하다.

## Experiments

### Which Weight Matrices in Transformer should We Apply LoRA to?

어떤 유형의 가중치를 LoRA로 적응시키는 것이 downstream task에서 가장 좋은 성능을 내는지를 실험하였다.

<center><img src='{{"/assets/images/논문리뷰/LoRA-2.png" | relative_url}}' width="90%"></center>
<br>
$\Delta W_q$ 또는 $\Delta W_k$ 하나만 적응시키는 경우 성능이 크게 떨어지는 반면, $W_q$와 $W_v$를 함께 적응시키는 경우 가장 좋은 성능을 보인다.

이는 rank 4로도 충분한 정보를 담을 수 있으므로, 하나의 가중치에 높은 rank를 할당하는 것보다 여러 가중치를 적응시키는 것이 더 효과적임을 보여준다.

### What is the Optimal Rank $r$ for LoRA?

Rank $r$이 성능에 미치는 영향을 실험하였다.

<center><img src='{{"/assets/images/논문리뷰/LoRA-3.png" | relative_url}}' width="100%"></center>
<br>
LoRA는 아주 작은 rank $r$만으로도 경쟁력 있는 성능을 내며, 이는 $\Delta W$가 매우 낮은 intrinsic rank를 가진다는 것을 의미한다.

**Subspace similarity between different $r$**

서로 다른 rank 간의 subspace 유사도를 분석하였다.

$\text{rank}=8$로 학습한 적응 행렬 $A_{r=8}$과 $\text{rank}=64$로 학습한 $A_{r=64}$에 대해 SVD 분해를 수행하고, 각각의 right-singular 행렬 $U_{A_{r=8}}$와 $U_{A_{r=64}}$를 얻었다.

$U_{A_{r=8}}$의 상위 $i$개 특이벡터가 $U_{A_{r=64}}$의 상위 $j$개 특이벡터 공간에 얼마나 포함되어 있는지를 계산하기 위해, Grassmann distance를 기반으로 subspace의 유사도를 측정하였다.

$$
\phi(A_{r=8}, A_{r=64}, i, j) =
\frac{\lVert U_{A_{r=8}}^{i\top} U_{A_{r=64}}^{j} \rVert_F^2}{\min(i, j)} \in [0, 1]
$$

$\phi=1$이면 완전히 같은 공간, $\phi=0$이면 완전히 다른 공간을 의미한다.

<center><img src='{{"/assets/images/논문리뷰/LoRA-4.png" | relative_url}}' width="100%"></center>
<br>
세 번째와 네 번째 그림은 첫 번째와 두 번째 그림의 좌하단을 확대한 그림이다.

$A_{r=8}$과 $A_{r=64}$의 상위 1개의 특이벡터 방향은 서로 높은 유사도($\phi>0.5$)를 보이며, 나머지 특이벡터들은 거의 겹치지 않는다.

즉, 중요한 정보는 상위 몇 개의 특이벡터 (방향)에만 집중되어 있으며, 이는 유용한 정보는 매우 저차원 서브스페이스에 담겨 있고, 나머지는 학습 과정 중의 노이즈일 가능성이 높다는 것을 의미한다.

**Subspace similarity between different random seeds**

<center><img src='{{"/assets/images/논문리뷰/LoRA-5.png" | relative_url}}' width="100%"></center>
<br>
서로 다른 seed로 학습한 rank 64 모델들의 subspace 유사도도 분석하였다.

$\Delta W_q$는 $\Delta W_v$보다 더 많은 공통 특이벡터 방향을 공유하며, 이는 $\Delta W_q$가 더 높은 intrinsic rank를 가진다는 것을 의미한다.

비교를 위해 랜덤 Gaussian 행렬 두 개를 생성하여 유사도를 측정한 결과, 공통 방향이 거의 없었으며, 이는 위 실험 결과가 의미 있는 것임을 뒷받침한다.
