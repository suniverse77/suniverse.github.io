---
layout: single
title: "[λ…Όλ¬Έλ¦¬λ·°] LoRA: Low-Rank Adaptation of Large Language Models"
last_modified_at: 2025-02-01
categories: ["λ…Όλ¬Έλ¦¬λ·°"]
tags: ["LLM"]
excerpt: "ICLR 2022"
use_math: true
toc: true
toc_sticky: true
---

**ICLR 2022** 
[[Paper]](https://arxiv.org/abs/2106.09685)

<details>
<summary><font color='#FF8C00'>π“ Summary</font></summary>
<div markdown="1">
<br>
νμΈνλ‹μ€ μ „μ²΄ κ°€μ¤‘μΉλ¥Ό μ—…λ°μ΄νΈν•΄μ•Ό ν•μ§€λ§, μ‹¤μ λ΅ μ—…λ°μ΄νΈν•  λ• ν•„μ”ν• κ°€μ¤‘μΉλ” low-rankλ΅ ν‘ν„μ΄ κ°€λ¥ν•λ‹¤.
- κ°€μ¤‘μΉ λ³€ν™”λ‰μ„ 2κ°μ μ‘μ€ ν–‰λ ¬λ΅ λ¶„ν•΄ β†’ $\Delta W=BA$
- μ›λ κ°€μ¤‘μΉ $W_0$λ¥Ό κ³ μ •ν•κ³ , $B$μ™€ $A$λ§ ν•™μµ

μ¶”λ΅  μ‹ κ°€μ¤‘μΉλ¥Ό $W = W_0 + BA$ ν•λ‚λ΅ ν•©μ³μ„ μ‚¬μ©ν•λ©΄ λκΈ° λ•λ¬Έμ— μ¶”λ΅  μ§€μ—°μ΄ μ—†λ‹¤.

</div>
</details>

## Introduction

**κΈ°μ΅΄ μ—°κµ¬λ“¤μ ν•κ³„μ **

- νμΈνλ‹μ μ£Όμ” λ‹¨μ μ€ μƒλ΅­κ² μƒμ„±λ λ¨λΈμ΄ μ›λ λ¨λΈκ³Ό λ™μΌν• μμ νλΌλ―Έν„°λ¥Ό κ°€μ§„λ‹¤λ” μ μ΄λ‹¤.
- μ΄λ¬ν• λ¬Έμ λ¥Ό μ™„ν™”ν•κΈ° μ„ν•΄ λ§μ€ μ—°κµ¬λ“¤μ΄ μΌλ¶€ νλΌλ―Έν„°λ§μ„ μ μ‘μ‹ν‚¤κ±°λ‚ μƒλ΅μ΄ μ‘μ—…μ„ μ„ν• μ™Έλ¶€ λ¨λ“μ„ ν•™μµν•λ” λ°©λ²•μ„ μ μ•ν–μ§€λ§, μ΄λ¬ν• λ°©λ²•λ“¤μ€ ν¨μ¨μ„±κ³Ό λ¨λΈ ν’μ§ μ‚¬μ΄μ νΈλ μ΄λ“μ¤ν”„λ¥Ό λ°μƒμ‹ν‚¨λ‹¤.

**μ μ•ν•λ” λ°©λ²•**

- λ§μ€ νλΌλ―Έν„°λ¥Ό κ°€μ§„ λ¨λΈλ“¤(over-parametrized models)μ€ μ‹¤μ λ΅ μ €μ°¨μ›μ— λ†“μ—¬μλ‹¤. β†’ μ΄λ¥Ό λ¨λΈμ΄ low intrinsic dimensionμ„ κ°€μ§„λ‹¤κ³  ν‘ν„
- LoRAμ—μ„λ” νμΈνλ‹μ—μ„ λ°μƒν•λ” κ°€μ¤‘μΉ ν–‰λ ¬ λ³€ν™” $\Delta W$λ” ν•„μ” μ΄μƒμΌλ΅ ν° ν–‰λ ¬μ΄λ©°, μ†μμ μ„ ν•κ²°ν•©μΌλ΅ ν‘ν„ν•  μ μλ‹¤κ³  κ°€μ •ν•μ€λ‹¤. β†’ μ΄λ¥Ό ν–‰λ ¬μ΄ low intrinsic rankλ¥Ό κ°€μ§„λ‹¤κ³  ν‘ν„
    
    λ”°λΌμ„, μ‚¬μ „ν•™μµλ μ›λ³Έ κ°€μ¤‘μΉ $W_0$λ” κ³ μ •μ‹ν‚¨ μ±„, λ³€ν™”ν•λ” dense layerμ κ°€μ¤‘μΉ $\Delta W$ λ¶€λ¶„λ§ low rank ν–‰λ ¬ λ¶„ν•΄ ν•νƒλ΅ ν•™μµν•λ‹¤.
    
- LoRAλ” μ•„λμ™€ κ°™μ€ μ¥μ μ„ κ°€μ§„λ‹¤.
    1. μ‚¬μ „ν•™μµλ λ¨λΈμ„ κ³µμ ν•λ” μ—¬λ¬ taskμ— λ€ν•΄μ„ μ €μ©λ‰ ν–‰λ ¬ $A$, $B$λ§ κµμ²΄ν•λ©΄ λλ―€λ΅, task κ°„ μ „ν™μ΄ ν¨μ¨μ μ΄λ‹¤.
    2. κ³„μ‚°ν•΄μ•Ό ν•  gradientκ°€ λ€ν­ μ¤„μ–΄λ“¤κΈ° λ•λ¬Έμ— λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ μ μ•½λλ‹¤. μ΄ λ•λ¶„μ— κΈ°μ΅΄ λ¨λΈμ„ ν•™μµν–μ„ λ•λ³΄λ‹¤ λ” λ‚®μ€ μ‚¬μ–‘μ ν•λ“μ›¨μ–΄μ—μ„λ„ ν•™μµμ΄ κ°€λ¥ν•λ‹¤.
    3. λ‹¨μν• μ„ ν• κµ¬μ΅° λ•λ¶„μ—, LoRAλ” ν•™μµλ ν–‰λ ¬μ„ κ³ μ •λ κ°€μ¤‘μΉμ™€ ν•©μ³ λ°°ν¬ν•  μ μμΌλ©°, μ¶”λ΅  μ§€μ—°μ΄ μ „ν€ μ—†λ‹¤.
    4. LoRAλ” κΈ°μ΅΄ λ°©λ²•λ“¤κ³Ό λ…λ¦½μ μΌλ΅ μ‘λ™ν•κΈ° λ•λ¬Έμ—, prefix-tuningκ³Ό κ°™μ€ λ‹¤λ¥Έ κΈ°λ²•λ“¤κ³Όλ„ μ‰½κ² κ²°ν•©ν•  μ μλ‹¤.

## Methods

### 1. Problem Statement

μ‚¬μ „ν•™μµλ autoregressive μ–Έμ–΄ λ¨λΈμ„ $P_\Phi(y\mid x)$λΌκ³  ν•  λ•, νμΈνλ‹ λ°©μ‹μ MLE λ©μ ν•¨μλ” μ•„λμ™€ κ°™λ‹¤.

$$
\underset{\Phi}{\max}~
\sum_{(x,y)\in\mathcal{Z}}\sum_{t=1}^{\lvert y\rvert}
\log(P_\Phi(y_t\mid x,y_{<t}))
$$

$\Phi_0$λ¥Ό μ‚¬μ „ν•™μµλ κ°€μ¤‘μΉλΌκ³  ν•  λ•, νμΈνλ‹μ—μ„ νλΌλ―Έν„° μ—…λ°μ΄νΈλ” $\Phi_0+\Delta\Phi$λ΅ ν‘ν„λλ‹¤. μ΄λ•, $\Phi_0$μ μ°¨μ›κ³Ό $\Delta\Phi$μ μ°¨μ›μ΄ κ°™κΈ° λ•λ¬Έμ— κ±°λ€ λ¨λΈμ—μ„λ” νμΈνλ‹ν•λ” κ²ƒμ΄ μ–΄λ µλ‹¤.

LoRAλ¥Ό μ΄μ©ν•΄ μΌλ¶€ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈν•λ” MLE λ©μ ν•¨μλ” μ•„λμ™€ κ°™λ‹¤.

$$
\underset{\Theta}{\max}~
\sum_{(x,y)\in\mathcal{Z}}\sum_{t=1}^{\lvert y\rvert}
\log(P_{\Phi_0+\Delta\Phi(\Theta)}(y_t\mid x,y_{<t}))
$$

LoRAμ—μ„μ νλΌλ―Έν„° μ—…λ°μ΄νΈλ” $\Phi_0+\Delta\Phi(\Theta)$λ΅ ν‘ν„λλ‹¤. μ—…λ°μ΄νΈν•λ” νλΌλ―Έν„°μ ν¬κΈ° $\lvert\Theta\rvert$λ” μ „μ²΄ νλΌλ―Έν„°μ ν¬κΈ° $\lvert\Phi_0\rvert$μ 0.01%λΌκ³  ν•λ‹¤.

### 2. Low-Rank-Parametrized Update Matrices

<center><img src='{{"/assets/images/λ…Όλ¬Έλ¦¬λ·°/LoRA-1.png" | relative_url}}' width="40%"></center>
<br>
μ‹ κ²½λ§μ—λ” ν–‰λ ¬ κ³± μ—°μ‚°μ„ μν–‰ν•λ” μ—¬λ¬ κ°μ dense layerκ°€ μ΅΄μ¬ν•λ©°, dense layerμ κ°€μ¤‘μΉ ν–‰λ ¬μ€ μΌλ°μ μΌλ΅ full-rankλ¥Ό κ°€μ§„λ‹¤.

κΈ°μ΅΄ μ—°κµ¬μ—μ„λ” νΉμ • μ‘μ—…μ— μ μ‘(adaptation)ν•  λ•, μ‚¬μ „ν•™μµλ μ–Έμ–΄ λ¨λΈμ΄ low intrinsic dimensionμ„ κ°€μ§€λ©°, random projectionμ„ ν†µν•΄ λ” μ‘μ€ subspaceλ΅ μ®κ²¨λ„ ν¨μ¨μ μΌλ΅ ν•™μµν•  μ μμμ„ λ³΄μ€λ‹¤.

μ΄λ¥Ό λ°”νƒ•μΌλ΅, LoRAμ—μ„λ” μ μ‘ κ³Όμ •μ—μ„ κ°€μ¤‘μΉ μ—…λ°μ΄νΈ λν• low intrinsic rankλ¥Ό κ°€μ§„λ‹¤κ³  κ°€μ •ν•μ€μΌλ©°, μ—…λ°μ΄νΈ κ°€μ¤‘μΉ ν–‰λ ¬ $\Delta W$λ¥Ό low rank decompositionν•μ€λ‹¤. 

$$
W_0+\Delta W=W_0+BA
\\
W_0\in\mathbb{R}^{d\times k},~B\in\mathbb{R}^{d\times r},~A\in\mathbb{R}^{r\times k},~r\leq\min(d,k)
$$

ν•™μµ μ¤‘μ—λ” $W_0$λ¥Ό κ³ μ •ν•κ³ , $A$μ™€ $B$λ§ ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°λ΅ λ‘”λ‹¤.

$\Delta W$μ™€ $BA$ λ¨λ‘ μ°¨μ›μ€ $d\times k$μ΄μ§€λ§, μ—…λ°μ΄νΈν•  νλΌλ―Έν„° κ°μκ°€ λ‹¤λ¥΄λ‹¤.

- $\Delta W$λ” full-rankμ΄κΈ° λ•λ¬Έμ— μ—…λ°μ΄νΈν•  νλΌλ―Έν„° κ°μκ°€ $d\times k$κ°μ΄λ‹¤.
- $B$μ™€ $A$λ” low-rankμ΄κΈ° λ•λ¬Έμ— μ—…λ°μ΄νΈν•  νλΌλ―Έν„° κ°μκ°€ $r(d+k)$κ°μ΄λ‹¤.

Forward processλ” μ•„λμ™€ κ°™μ΄ μ •μλλ‹¤.

$$
h=(W_0+\Delta W)x=W_0x+BAx
$$

ν–‰λ ¬ $A$λ” random Gaussian $\mathcal{N}(0,\sigma^2)$μΌλ΅, ν–‰λ ¬ $B$λ” 0μΌλ΅ μ΄κΈ°ν™”ν•μ—¬ ν•™μµ μ΄κΈ°μ—λ” $\Delta W=0$μ΄ λλ„λ΅ ν•λ‹¤.

μ΄ν›„ $\Delta Wx$μ— μ¤μΌ€μΌλ§ μƒμ $\frac{\alpha}{r}$λ¥Ό κ³±ν•λ‹¤. ($\alpha$λ” μƒμ)

#### A Generalization of Full Fine-tuning

μΌλ°μ μΈ νμΈνλ‹μ€ μ‚¬μ „ν•™μµλ νλΌλ―Έν„°μ μΌλ¶€λ¶„λ§ ν•™μµν•λ‹¤. LoRAλ” λ” λ‚μ•„κ°€, μ—…λ°μ΄νΈλλ” κ°€μ¤‘μΉ ν–‰λ ¬μ΄ full-rankμΌ ν•„μ”κ°€ μ—†λ„λ΅ ν•λ‹¤.

$r$μ„ μ‚¬μ „ν•™μµλ κ°€μ¤‘μΉ ν–‰λ ¬μ rankμ™€ κ°™κ² μ„¤μ •ν•λ©΄, μ „μ²΄λ¥Ό νμΈνλ‹ν•λ” λ°©μ‹κ³Ό λ™μΌν• μ„±λ¥μ„ λ‚Ό μ μλ‹¤.

#### No Additional Inference Latency

LoRAλ¥Ό μ μ©ν•λ©΄, κ°€μ¤‘μΉλ¥Ό ν•©μ³μ„ $W=W_0+BA$λ΅ λ―Έλ¦¬ λ§λ“¤μ–΄λ‘ μ μλ‹¤. μ΄λ ‡κ² ν•λ©΄ μ¶”λ΅ ν•  λ• λ§¤λ² $BAx$λ¥Ό κ³„μ‚°ν•μ§€ μ•κ³  ν•λ‚μ ν–‰λ ¬κ³± $Wx$λ§ μν–‰ν•λ―€λ΅, μ¶”λ΅  μ†λ„κ°€ λλ ¤μ§€μ§€ μ•λ”λ‹¤.

λ‹¤λ¥Έ taskλ΅ μ „ν™ν•΄μ•Ό ν•  λ•λ” $BA$λ¥Ό λΉΌκ³  λ‹¤λ¥Έ $B'A'$λ¥Ό λ”ν•λ©΄ λλ―€λ΅ memory overheadκ°€ κ±°μ μ—†λ‹¤.

λ”°λΌμ„ νμΈνλ‹λ λ¨λΈκ³Ό λΉ„κµν•΄ inference λ• μ¶”κ°€μ μΈ latencyκ°€ λ°μƒν•μ§€ μ•μμ„ λ³΄μ¥ν•  μ μλ‹¤.

### 3. Applying LoRA to Transformer

νΈλμ¤ν¬λ¨Έμ self-attention λ¨λ“μ—λ” 4κ°μ κ°€μ¤‘μΉ ν–‰λ ¬ $W_q,W_k,W_v,W_o$κ°€ μκ³ , MLP λ¨λ“μ—λ” 2κ°μ κ°€μ¤‘μΉ ν–‰λ ¬μ΄ μ΅΄μ¬ν•λ‹¤.

λ³Έ μ—°κµ¬μ—μ„λ” λ‹¨μν•¨κ³Ό νλΌλ―Έν„° ν¨μ¨μ„±μ μ΄μ λ΅ MLP λ¨λ“μ€ κ³ μ •μ‹ν‚¤κ³ , attention λ¨λ“μ κ°€μ¤‘μΉλ§ LoRAλ΅ μ μ‘μ‹μΌ°λ‹¤.

#### Practical Benefits and Limitations

κ°€μ¥ ν° μ΄μ μ€ λ©”λ¨λ¦¬μ™€ μ €μ¥ κ³µκ°„ μ‚¬μ©λ‰μ κ°μ† λ° task κ°„μ μ „ν™μ΄ νΈλ¦¬ν•λ‹¤λ” μ μ΄λ‹¤.

μ„λ΅ λ‹¤λ¥Έ task μƒν”λ“¤μ„ ν•λ‚μ λ°°μΉλ΅ λ¬¶μ—μ„ λ•, κ° taskμ— λ€μ‘ν•λ” $BA$λ¥Ό λ―Έλ¦¬ ν•©μ³ ν•λ‚μ $W$λ΅ μ²λ¦¬ν•λ” κ²ƒμ΄ μ–΄λ µκΈ° λ•λ¬Έμ— λ°°μΉ λ‚΄ μƒν” λ³„λ΅ λ¶„λ¦¬λ μ—°μ‚°μ΄ ν•„μ”ν•λ‹¤.

## Experiments

### Which Weight Matrices in Transformer should We Apply LoRA to?

μ–΄λ–¤ μ ν•μ κ°€μ¤‘μΉλ¥Ό LoRAλ΅ μ μ‘μ‹ν‚¤λ” κ²ƒμ΄ downstream taskμ—μ„ κ°€μ¥ μΆ‹μ€ μ„±λ¥μ„ λ‚΄λ”μ§€λ¥Ό μ‹¤ν—ν•μ€λ‹¤.

<center><img src='{{"/assets/images/λ…Όλ¬Έλ¦¬λ·°/LoRA-2.png" | relative_url}}' width="90%"></center>
<br>
$\Delta W_q$ λλ” $\Delta W_k$ ν•λ‚λ§ μ μ‘μ‹ν‚¤λ” κ²½μ° μ„±λ¥μ΄ ν¬κ² λ–¨μ–΄μ§€λ” λ°λ©΄, $W_q$μ™€ $W_v$λ¥Ό ν•¨κ» μ μ‘μ‹ν‚¤λ” κ²½μ° κ°€μ¥ μΆ‹μ€ μ„±λ¥μ„ λ³΄μΈλ‹¤.

μ΄λ” rank 4λ΅λ„ μ¶©λ¶„ν• μ •λ³΄λ¥Ό λ‹΄μ„ μ μμΌλ―€λ΅, ν•λ‚μ κ°€μ¤‘μΉμ— λ†’μ€ rankλ¥Ό ν• λ‹Ήν•λ” κ²ƒλ³΄λ‹¤ μ—¬λ¬ κ°€μ¤‘μΉλ¥Ό μ μ‘μ‹ν‚¤λ” κ²ƒμ΄ λ” ν¨κ³Όμ μ„μ„ λ³΄μ—¬μ¤€λ‹¤.

### What is the Optimal Rank $r$ for LoRA?

Rank $r$μ΄ μ„±λ¥μ— λ―ΈμΉλ” μν–¥μ„ μ‹¤ν—ν•μ€λ‹¤.

<center><img src='{{"/assets/images/λ…Όλ¬Έλ¦¬λ·°/LoRA-3.png" | relative_url}}' width="100%"></center>
<br>
LoRAλ” μ•„μ£Ό μ‘μ€ rank $r$λ§μΌλ΅λ„ κ²½μλ ¥ μλ” μ„±λ¥μ„ λ‚΄λ©°, μ΄λ” $\Delta W$κ°€ λ§¤μ° λ‚®μ€ intrinsic rankλ¥Ό κ°€μ§„λ‹¤λ” κ²ƒμ„ μλ―Έν•λ‹¤.

**Subspace similarity between different $r$**

μ„λ΅ λ‹¤λ¥Έ rank κ°„μ subspace μ μ‚¬λ„λ¥Ό λ¶„μ„ν•μ€λ‹¤.

$\text{rank}=8$λ΅ ν•™μµν• μ μ‘ ν–‰λ ¬ $A_{r=8}$κ³Ό $\text{rank}=64$λ΅ ν•™μµν• $A_{r=64}$μ— λ€ν•΄ SVD λ¶„ν•΄λ¥Ό μν–‰ν•κ³ , κ°κ°μ right-singular ν–‰λ ¬ $U_{A_{r=8}}$μ™€ $U_{A_{r=64}}$λ¥Ό μ–»μ—λ‹¤.

$U_{A_{r=8}}$μ μƒμ„ $i$κ° νΉμ΄λ²΅ν„°κ°€ $U_{A_{r=64}}$μ μƒμ„ $j$κ° νΉμ΄λ²΅ν„° κ³µκ°„μ— μ–Όλ§λ‚ ν¬ν•¨λμ–΄ μλ”μ§€λ¥Ό κ³„μ‚°ν•κΈ° μ„ν•΄, Grassmann distanceλ¥Ό κΈ°λ°μΌλ΅ subspaceμ μ μ‚¬λ„λ¥Ό μΈ΅μ •ν•μ€λ‹¤.

$$
\phi(A_{r=8}, A_{r=64}, i, j) =
\frac{\lVert U_{A_{r=8}}^{i\top} U_{A_{r=64}}^{j} \rVert_F^2}{\min(i, j)} \in [0, 1]
$$

$\phi=1$μ΄λ©΄ μ™„μ „ν κ°™μ€ κ³µκ°„, $\phi=0$μ΄λ©΄ μ™„μ „ν λ‹¤λ¥Έ κ³µκ°„μ„ μλ―Έν•λ‹¤.

<center><img src='{{"/assets/images/λ…Όλ¬Έλ¦¬λ·°/LoRA-4.png" | relative_url}}' width="100%"></center>
<br>
μ„Έ λ²μ§Έμ™€ λ„¤ λ²μ§Έ κ·Έλ¦Όμ€ μ²« λ²μ§Έμ™€ λ‘ λ²μ§Έ κ·Έλ¦Όμ μΆν•λ‹¨μ„ ν™•λ€ν• κ·Έλ¦Όμ΄λ‹¤.

$A_{r=8}$κ³Ό $A_{r=64}$μ μƒμ„ 1κ°μ νΉμ΄λ²΅ν„° λ°©ν–¥μ€ μ„λ΅ λ†’μ€ μ μ‚¬λ„($\phi>0.5$)λ¥Ό λ³΄μ΄λ©°, λ‚λ¨Έμ§€ νΉμ΄λ²΅ν„°λ“¤μ€ κ±°μ κ²ΉμΉμ§€ μ•λ”λ‹¤.

μ¦‰, μ¤‘μ”ν• μ •λ³΄λ” μƒμ„ λ‡ κ°μ νΉμ΄λ²΅ν„° (λ°©ν–¥)μ—λ§ μ§‘μ¤‘λμ–΄ μμΌλ©°, μ΄λ” μ μ©ν• μ •λ³΄λ” λ§¤μ° μ €μ°¨μ› μ„λΈμ¤νμ΄μ¤μ— λ‹΄κ²¨ μκ³ , λ‚λ¨Έμ§€λ” ν•™μµ κ³Όμ • μ¤‘μ λ…Έμ΄μ¦μΌ κ°€λ¥μ„±μ΄ λ†’λ‹¤λ” κ²ƒμ„ μλ―Έν•λ‹¤.

**Subspace similarity between different random seeds**

<center><img src='{{"/assets/images/λ…Όλ¬Έλ¦¬λ·°/LoRA-5.png" | relative_url}}' width="100%"></center>
<br>
μ„λ΅ λ‹¤λ¥Έ seedλ΅ ν•™μµν• rank 64 λ¨λΈλ“¤μ subspace μ μ‚¬λ„λ„ λ¶„μ„ν•μ€λ‹¤.

$\Delta W_q$λ” $\Delta W_v$λ³΄λ‹¤ λ” λ§μ€ κ³µν†µ νΉμ΄λ²΅ν„° λ°©ν–¥μ„ κ³µμ ν•λ©°, μ΄λ” $\Delta W_q$κ°€ λ” λ†’μ€ intrinsic rankλ¥Ό κ°€μ§„λ‹¤λ” κ²ƒμ„ μλ―Έν•λ‹¤.

λΉ„κµλ¥Ό μ„ν•΄ λλ¤ Gaussian ν–‰λ ¬ λ‘ κ°λ¥Ό μƒμ„±ν•μ—¬ μ μ‚¬λ„λ¥Ό μΈ΅μ •ν• κ²°κ³Ό, κ³µν†µ λ°©ν–¥μ΄ κ±°μ μ—†μ—μΌλ©°, μ΄λ” μ„ μ‹¤ν— κ²°κ³Όκ°€ μλ―Έ μλ” κ²ƒμ„μ„ λ’·λ°›μΉ¨ν•λ‹¤.
