---
layout: single
title: "[논문리뷰] Classifier-Free Diffusion Guidance"
last_modified_at: 2025-03-17
categories: ["논문리뷰"]
tags: ["Diffusion", "Image Generation"]
excerpt: "NeurIPS Workshop 2021"
use_math: true
toc: true
toc_sticky: true
---

**NeurIPS Workshop 2021** 
[[Paper]](https://arxiv.org/abs/2207.12598)

<details>
<summary><font color='#FF8C00'>📝 Summary</font></summary>
<div markdown="1">
<br>


</div>
</details>
<br>
<center><img src='{{"/assets/images/논문리뷰/CFG-0.png" | relative_url}}' width="100%"></center>

## Introduction

**기존 연구들의 한계점**

Classifier guidance는 아래의 문제점이 있다.

- 추가적인 classifier 학습이 필요하기 때문에 diffusion 모델의 학습 파이프라인을 복잡하게 만듦
- 사용하는 classifier는 노이즈가 섞인 데이터 위에서 학습되어야 하므로, 일반적인 사전학습된 classifier를 바로 사용할 수 없음
- FID, IS 등의 metric은 사전학습된 classifier를 기반으로 평가하는데, 이 방식은 score를 직접적으로 더하면서 클래스 방향으로 샘플링을 유도하기 때문에 classifier가 잘 맞추게끔 이미지에 작은 조작을 반복하는 것으로 볼 수 있다.

    따라서 진짜 좋은 이미지를 만든 것인지, classifier를 잘 속인 결과인지가 불분명하다.

**제안하는 방법**

분류기를 사용하지 않는 guidance 기법을 제안한다.

- Conditional 디퓨전 모델의 socre 추정값과 unconditional 디퓨전 모델의 score 추정값을 혼합한다.

## Methods

GAN이나 flow-based 모델과 같은 생성 모델은 샘플링 때 입력 노이즈의 분산 또는 범위를 줄임으로써 truncated sampling 또는 low temperature sampling을 수행할 수 있다.

이러한 방식은 샘플의 다양성을 줄이지만, 각 샘플의 품질을 높일 수 있다.

그러나 위의 방식을 디퓨전 모델에 적용하기 위해, score를 스케일링하거나 reverse process에서 가우시안 노이즈의 분산을 줄이는 방식을 사용하면 오히려 저품질의 샘플을 생성하게 만든다.

### 1. Classifier Guidance

디퓨전 모델에서 truncation과 유사한 효과를 얻기 위해 [Diffusion]() 논문에서는 classifier guidance 기법을 도입하였다.

이 기법에서는 디퓨전 모델의 score ${\boldsymbol\epsilon}\_\theta(\mathbf{z}\_\lambda,\mathbf{c})\approx\sigma\_\lambda\nabla\_{\mathbf{z}\_\lambda}\log p\_\theta(\mathbf{z}\_\lambda\mid\mathbf{c})$에 auxiliary classifier 모델 $p\_\theta(\mathbf{c}\mid\mathbf{z}_\lambda)$의 log likelihood gradient를 포함시킨다.

$$
\tilde{\boldsymbol\epsilon}_\theta(\mathbf{z}_\lambda,\mathbf{c})
={\boldsymbol\epsilon}_\theta(\mathbf{z}_\lambda,\mathbf{c})-w\sigma_\lambda\nabla_{\mathbf{z}_\lambda}\log p_\theta(\mathbf{c}\mid\mathbf{z}_\lambda)
\approx\sigma_\lambda\nabla_{\mathbf{z}_\lambda}[\log p_\theta(\mathbf{z}_\lambda\mid\mathbf{c})+w\log p_\theta(\mathbf{c}\mid\mathbf{z}_\lambda)]
$$

### 2. Classifier-Free Guidance



## Experiments