---
layout: single
title: "[논문리뷰] Denoising Diffusion Probabilistic Models"
last_modified_at: 2025-03-11
categories: ["논문리뷰"]
tags: ["Image Generation"]
excerpt: "NeurIPS 2020"
use_math: true
toc: true
toc_sticky: true
---

> [[Paper]](https://arxiv.org/abs/2006.11239)
>
> **NeurIPS 2020**

## Introduction

## Methods

### Forward process

#### ㄴ

Forward process는 Gaussian Noise를 점진적으로 주입하여 최종적으로 $N(0,\text{I})$로 변형하는 과정이다.

$$
q(\mathbf x_{1:T}\mid\mathbf x_0):=\prod_{t=1}^Tq(\mathbf x_t\mid\mathbf x_{t-1})
$$

- Markov로 이루어진 forward process → $\textbf{x}_t$는 바로 이전 상태 $\textbf{x}_{t-1}$에만 의존
- MHVAE 공식에서 학습 파라미터가 사라진 형태
- $z$ 기호를 사용하지 않음 → 초기 step도 $\prod$ 안에 넣을 수 있음

$$
q(\mathbf x_t\mid\mathbf x_{t-1}):=
\mathcal{N}(\mathbf x_t;\sqrt{1-\beta_t}\mathbf x_{t-1},\beta_t\mathbf I)
$$

- $q(\textbf{x}_t|\textbf{x}_{t-1})$는 가우시안 분포로 정의된 Forward Process의 한 단계
- 주입하는 노이즈의 강도를 조절하기 위해 하이퍼파라미터 $\beta_t$ (variance schedule)를 이용
- 노이즈의 level은 step별로 편차가 너무 크면 안됨

#### 실제 사용하는 방식

초기 데이터에서 임의의 time step $t$로 한 번에 갈 수 있는 수식이 아래와 같이 closed form으로 존재한다.

$$
q(\mathbf x_t\mid\mathbf x_0)=\mathcal{N}(\mathbf x_t;\sqrt{\bar\alpha_t}\mathbf x_{t-1},(1-\bar\alpha_t)\mathbf I)
$$

$\alpha_t:=1-\beta_t$ , $\bar\alpha_t:=\prod_{s=1}^t\alpha_s$으로 정의된다.

$$
\mathbf
x_t=\sqrt{\bar{\alpha}_t}\mathbf x_0+\sqrt{(1-\bar{\alpha}_t)}\boldsymbol\epsilon
~,~
\boldsymbol\epsilon \sim \mathcal N(\mathbf0,\textbf{I})
$$

역전파가 가능하기 하기 위해서 샘플링 과정을 reparametrization trick을 사용한 위와 같이 변형한다.

$\epsilon$은 각 단계에서 추가되는 가우시안 노이즈이다.

#### Variance scheduler

time step $t$에 따라 추가되는 노이즈의 분산을 조절하는 매커니즘으로, 스케줄 파라미터 $\beta_t$를 통해 정의한다.

- 초기 단계(작은 $t$)에서는 데이터의 특성을 보존하면서 노이즈를 천천히 추가하는 것이 중요하기 때문에 적은 노이즈를 추가(작은 $\beta_t$)
- 후반 단계(큰 $t$)에서는 이미 데이터가 많이 왜곡되어 있기 때문에 빠른 수렴을 위해 큰 노이즈를 추가(큰 $\beta_t$)
- Linear schedule, Cosine schedule을 사용함

### Reverse process



## Experiments
