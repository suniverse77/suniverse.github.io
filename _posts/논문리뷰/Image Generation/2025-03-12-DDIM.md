---
layout: single
title: "[논문리뷰] Denoising Diffusion Implicit Models"
last_modified_at: 2025-03-12
categories: ["논문리뷰"]
tags: ["Image Generation"]
excerpt: "ICLR 2021"
use_math: true
toc: true
toc_sticky: true
---

**ICLR 2021** [[Paper]](https://arxiv.org/abs/2010.02502)

## Introduction

**기존 연구들의 한계점**

- DDPM과 같은 iterative generative model들은 고품질 샘플을 생성하기 위해 많은 반복 단계를 필요로 한다.
- 반면 GAN은 단 한 번의 네트워크 통과만으로 샘플을 생성하므로 훨씬 빠르다.

**제안하는 방법**

- DDPM과 동일한 학습 목적 함수를 사용하지만, non-Markovian 확산 과정을 도입해 짧은 생성 경로를 가능하게 한다.
- 같은 초기 latent에서 생성된 결과들이 구조적으로 유사한 일관성을 가진다.

## Methods

### 1. Variational Inference for non-Markovian Forward Process

저자들은 DDPM의 목적 함수 $L_\gamma$는 joint 분포 $q(\mathbf x_{1:T}\mid\mathbf x_0)$가 아니라 marignal 분포 $q(\mathbf x_t\mid\mathbf x_0)$에만 의존한다는 것을 발견하였다.

동일한 marignal 분포를 갖는 joint 분포가 여러 가지가 존재하므로, 기존의 Markovian 추론 과정을 대체할 수 있는 새로운 non-Markovian 추론 분포를 찾을 수 있다.

<center><img src='{{"/assets/images/논문리뷰/DDIM-1.png" | relative_url}}' width="100%"></center>

#### Non-Markovian Forward Process

기존 DDPM과 동일한 marginal 분포를 갖는 non-Markovian joint 분포를 아래와 같이 새롭게 정의하였다.

$$
q_\sigma(\mathbf{x}_{1:T} \mid \mathbf{x}_0) :=
q_\sigma(\mathbf{x}_T \mid \mathbf{x}_0) \prod_{t=2}^{T} q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)
$$

모든 $t$에 대해서 $q_\sigma(\mathbf{x}_T \mid \mathbf{x}_0) = \mathcal{N}(\sqrt{\alpha_t} \mathbf{x}_0,\ (1 - \alpha_t) \mathbf{I})$의 식을 만족하기 위해서 평균 함수는 아래와 같이 유도되었다.

$$
q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) =
\mathcal{N}\bigg(
\sqrt{\bar\alpha_{t-1}}\, \mathbf{x}_0
+ \sqrt{1 - \bar\alpha_{t-1} - \sigma_t^2} \cdot
\frac{\mathbf{x}_t - \sqrt{\bar\alpha_t} \mathbf{x}_0}{\sqrt{1 - \bar\alpha_t}},
\sigma_t^2 \mathbf{I}
\bigg)
$$

베이즈 정리에 의해 forward process는 아래와 같이 표현할 수 있으며, 이 또한 가우시안 분포이다.

$$
q_\sigma(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0) =
\frac{q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \, q_\sigma(\mathbf{x}_t \mid \mathbf{x}_0)}{q_\sigma(\mathbf{x}_{t-1} \mid \mathbf{x}_0)}
$$

$\mathbf{x}\_t$가 $\mathbf{x}\_{t-1}$과 $\mathbf{x}\_0$에 의존하므로, forward process는 더이상 Markovian이 아니다.

#### Generative Process and Unified Variational Inference Objective

학습 가능한 생성 과정 $p_\theta(\mathbf{x}_{0:T})$를 정의한다.

각 $p_\theta^{(t)}(\mathbf{x}_{t-1}\mid\mathbf{x}_t)$는 $q_\sigma(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0)$에 대한 지식을 활용한다. 
직관적으로, 노이즈가 추가된 관측값 $\mathbf{x}_t$가 주어졌을 때, 먼저 그에 대응되는 $\mathbf{x}\_0$를 예측하고, $q_\sigma(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_0)$를 이용해 $\mathbf{x}_{t-1}$을 생성한다.

$$
\mathbf x_t=\sqrt{\bar{\alpha}_t}\mathbf x_0+\sqrt{(1-\bar{\alpha}_t)}\boldsymbol\epsilon
$$

위의 식은 DDPM에서 정의된 forward sampling을 나타낸다.

모델은 $\epsilon_\theta(\mathbf{x}_t,t)$는 $\mathbf{x}_t$로부터 노이즈 $\epsilon_t$를 예측하기 때문에 위의 식을 아래와 같이 작성할 수 있다.

$$
f_\theta^{(t)}(\mathbf{x}_t):=\frac{\mathbf{x}_t-\sqrt{1-\bar\alpha_t}\dot\boldsymbol\epsilon_\theta^{(t)}(\mathbf{x}_t)}{\sqrt{\bar\alpha_t}}
$$

생성 과정 $p_\theta(\mathbf{x}_{0:T})$는 아래와 같이 정의된다.

$$
p_\theta^{(t)}(x_{t-1} \mid x_t) = 
\begin{cases}
\mathcal{N}(f_\theta^{(t)}(x_1),\, \sigma_1^2 \mathbf{I}) & \text{if } t = 1 \\
q_\sigma(x_{t-1} \mid x_t,\, f_\theta^{(t)}(x_t)) & \text{if } t > 1
\end{cases}
$$

### 2. Sampling from Generalized Generative Process

<center><img src='{{"/assets/images/논문리뷰/DDIM-2.png" | relative_url}}' width="80%"></center>

#### Denoising Diffusion Implicit Models

#### Accelerated Generation Process

## Experiments

