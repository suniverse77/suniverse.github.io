---
layout: single
title: "[최적화] Lagrange Multiplier"
last_modified_at: 2024-11-30
categories: ["인공지능 수학"]
tags: ["최적화"]
excerpt: "라그랑주"
use_math: true
toc: true
toc_sticky: true
---

## Max-min inequality

$$
\underset{\lambda}{\max}~\underset{x}{\min} ~f(x,\lambda)\leq\underset{x}{\min}~\underset{\lambda}{\max}~f(x,\lambda)
$$

임의의 함수 $f(x,\lambda)$에 대해 최대화하고 최소화하는 것이 최소화하고 최대화하는 것보다 항상 크다.

<details>
<summary><font color='blue'>공식 유도</font></summary>
<div markdown="1">

1. $g(x,\lambda):=\underset{x}{\min} ~f(x,\lambda)$
2. $g(x,\lambda)\leq f(x,\lambda)$
3. $\underset{\lambda}{\max} ~g(x,\lambda)\leq\underset{\lambda}{\max} ~f(x,\lambda)$
4. $\underset{\lambda}{\max} ~g(x,\lambda)\leq\underset{x}{\min} ~\underset{\lambda}{\max} ~f(x,\lambda)$

</div>
</details>

## Lagrangian

![Figure 1](/assets/images/인공지능수학/6-2. Figure1.png){: style="display:block; margin:0 auto; width: 50%; height: 50%;"}

Primal problem이 위와 같이 주어졌을 때, 아래와 같이 원래의 목적 함수와 제약 조건을 결합한 함수를 Lagrangian이라고 한다.

![Figure 2](/assets/images/인공지능수학/6-2. Figure2.png){: style="display:block; margin:0 auto; width: 50%; height: 50%;"}

제약 조건이 있는 최적화 문제를 다루기 쉽게 만들고, Dual 문제를 정의하기 위해 사용한다.

이때 $\lambda$와 $\nu$를 lagrange multiplier라고 부르며, 부등식 조건에 대한 승수에는 항상 $\lambda_i\geq0$의 조건이 붙음

### Dual problem

원래의 최적화 문제 (primal problem)를 직접 푸는 대신, 다른 문제 (dual problem)를 풀어서 해를 유도하거나 경계를 줄 수 있음

Primal은 원래의 최적화 문제, Dual은 라그랑지안을 통해 정의한 새로운 최적화 문제임

### Lagrangian Dual problem

## Duality gap

## KKT conditions



최적화 문제가 위와 같이 주어져있을 때, KKT 조건은 아래와 같다.

**1. Stationarity**
        
$\nabla_\mathbf xf(\mathbf x^*)+
\sum_{i=1}^m\lambda_i^*\nabla_\mathbf xg_i(\mathbf x^*)+
\sum_{j=1}^p\mu_j^*\nabla_\mathbf xh_j(\mathbf x^*)
=0$

라그랑지안의 gradient가 0이 되는지점이 optimal임
        
**2. Primal constraints**
        
$g_i(\mathbf x^*)\leq0
~,~
h_j(\mathbf x^*)=0$

Primal 문제에서의 제약 조건
        
**3. Dual constraints**
        
$\lambda_i^*\geq0$

Dual 문제에서의 제약 조건
        
**4. Complementary slackness**
        
$\lambda_i^*g_i(\mathbf x^*)=0$

<details>
<summary><font color='purple'>Complementary slackness</font></summary>
<div markdown="1">

**1. $g_i(\mathbf x^*)<0~\rightarrow~\lambda_i^*=0$**

<figure style="text-align: center;">
  <img src='{{ "/assets/images/인공지능수학/6-2. Figure6.png" | relative_url }}' style="display: block; margin: 0 auto; width: 60%;">
  <figcaption>출처: https://www.cnblogs.com/pingzeng/p/7019221.html</figcaption>
</figure>

- Optimal $\bold x^*$가 제약 조건의 영역 내에 있는 경우 → $g_i(\bold x^*)<0$
- 원래 목적 함수의 최적해가 이미 제약 조건 영역 내에 있었음
- 따라서, 해당 제약 조건을 없애더라도 기존의 최적해가 변하지 않음

---

**2. $\lambda_i^*>0~\rightarrow~g_i(\mathbf x^*)=0$**

<figure style="text-align: center;">
  <img src='{{ "/assets/images/인공지능수학/6-2. Figure7.png" | relative_url }}' style="display: block; margin: 0 auto; width: 60%;">
  <figcaption>출처: https://www.cnblogs.com/pingzeng/p/7019221.html</figcaption>
</figure>

- Optimal $\bold x^*$가 제약 조건의 경계에 있는 경우 → $g_i(\bold x^*)=0$
- 원래 목적 함수의 최적해가 제약 조건 영역의 바깥에 있었음
- 따라서, 제약 조건이 존재할 때의 최적해는 기존의 최적해와 다름
- 이 경우에 최적해는 일반적으로 제약 조건의 경계에 위치함

</div>
</details>

### lagrange multiplier의 의미

부등식에 대한 lagrange multiplier $\lambda$를 shadow price라고 부르며, <mark style='background-color: fff5b1'>목적 함수의 최적값이 제약 조건에 대해 얼마나 민감한지</mark>를 나타낸다.

$\lambda$가 클수록 해당 제약 조건이 목적 함수에 더 민감하게 영향을 준다는 뜻이며, $\lambda$를 조금만 완화해도 목적 함수가 크게 개선된다는 뜻으로 해석할 수도 있다.

<details>
<summary><font color='red'>Example</font></summary>
<div markdown="1">

공장에서의 물건 생산에서 아래의 조건이 있을 때, 원자재 1kg을 더 사용할 수 있다면 이윤이 얼마나 증가하는가?

- 제약 조건: 원자재는 최대 100kg 사용 가능
- 현재 최적 상태: 이윤 500달러, 원자재는 딱 100kg 사용 중

---

1kg 더 주었더니 이윤이 5달러 증가했다면 이 5달러가 바로 shadow price임

</div>
</details>
