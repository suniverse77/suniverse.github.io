---
layout: single
title: "[확률] Maximum Likelihood Estimation"
last_modified_at: 2024-11-27
categories: ["인공지능 수학"]
tags: ["확률"]
excerpt: "베이즈 이론과 Likelihood 및 MLE와 MAP"
use_math: true
toc: true
toc_sticky: true
---

## Bayes' Theorem

$$
P(B|A)=\frac{P(B)P(A|B)}{P(A)}
$$

- prior와 likelihood를 이용해 posterior를 구하는 과정
- 어떤 사건이 발생한 후, 그 원인이 될 가능성이 있는 사건의 확률을 갱신하는 공식
    - $A$가 결과, $B$가 원인

- 사전 확률 (Prior) | $P(B)$ | 과거 경험에 의해 원인 $B$가 일어날 초기 확률 |
- 우도 (Likelihood) | $P(A|B)$ | 원인 $B$에 의해 결과 $A$가 발생하는 조건부 확률 |
- 사후 확률 (Posterior) | $P(B|A)$ | 사건 $A$가 일어났다는 것을 알고, 그 결과가 $B$라는 원인으로부터 일어났을 확률 |
- 정규화 상수(Evidence) | $P(A)$ | 현재의 증거 ($A$의 사전 확률) |

## Likelihood

$$
L(\theta)=p(D|\theta)
$$

해당 파라미터가 주어진 확률 분포를 얼마나 잘 표현하는지를 나타내는 값이다.

- 확률 변수 $D$는 이미 주어져있기 때문에, likelihood는 $\theta$에 관한 함수임
- 주어진 데이터를 기반으로 파라미터의 적합도를 평가하는 함수이기 때문에 확률이라고 표현 안함
    
확률은 파라미터가 고정된 상태에서 데이터가 나올 가능성을 의미함
    

$$
\displaystyle
p(D|\theta)
=\prod_{i=1}^N p(x^{(i)}|\theta)
~,~\bigg(D=\{x^{(1)},~x^{(2)},\cdots ,~x^{(N)}\}\bigg)
$$

- 샘플 $D$의 각 데이터들이 i.i.d를 따른다고 가정하면 위와 같이 표현 가능

## Maximum Likelihood Estimation (MLE)

## Maximum a Posteriori Estimation (MAP)
